{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONb0bofPNDIXmdgh6VoG69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasinMakandar/AI-ML-18CSL76/blob/main/AI_ML_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install heuristicsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5kMg3wpCw7z",
        "outputId": "4fe57c87-119c-4981-c60c-3294ec3ccc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting heuristicsearch\n",
            "  Downloading heuristicsearch-0.1.1-py3-none-any.whl (5.5 kB)\n",
            "Installing collected packages: heuristicsearch\n",
            "Successfully installed heuristicsearch-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsd4EH4uhSqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67fd6f84-6a9c-49d9-9936-13e4d5d6b994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path\n",
            "A -> B -> D\n",
            "Cost\n",
            "0 -> 1 -> 6\n"
          ]
        }
      ],
      "source": [
        "from heuristicsearch.a_star_search import AStar\n",
        "\n",
        "adjacency_list = {\n",
        "    'A': [('B', 1), ('C', 3), ('D', 7)],\n",
        "    'B': [('D', 5)],\n",
        "    'C': [('D', 12)]\n",
        "}\n",
        "\n",
        "heuristics = {'A':1, 'B':1, 'C':1, 'D':1}\n",
        "\n",
        "graph = AStar(adjacency_list, heuristics)\n",
        "graph.apply_a_star(start='A',stop='D')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heuristicsearch.ao_star import AOStar\n",
        "print(\"Graph-1\")\n",
        "heuristic = {'A':1, 'B':6, 'C':12, 'D':10, 'E':4,'F':4,'G':5,'H':7}\n",
        "\n",
        "adjacency_list = {\n",
        "    'A': [[('B', 1), ('C', 1), ('D', 1)]],\n",
        "    'B': [[('G', 1), ('H',1)]],\n",
        "    # 'C': [[(('J', 1))]],\n",
        "    'D': [[('E', 1), ('F',1)]],\n",
        "    # 'G': [[(('I', 1))]],\n",
        "}\n",
        "\n",
        "graph = AOStar(adjacency_list, heuristic, 'A')\n",
        "graph.applyAOStar()\n"
      ],
      "metadata": {
        "id": "r4loAqT1habK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"CandidateElimination.csv\") as f:\n",
        "    csv_file = csv.reader(f)\n",
        "    data = list(csv_file)\n",
        "\n",
        "    s = data[1][:-1]\n",
        "    g = [['?' for i in range(len(s))] for j in range(len(s))]\n",
        "\n",
        "    for i in data:\n",
        "        if i[-1] == \"Yes\":\n",
        "            for j in range(len(s)):\n",
        "                if i[j] != s[j]:\n",
        "                    s[j] = '?'\n",
        "                    g[j][j] = '?'\n",
        "\n",
        "        elif i[-1] == \"No\":\n",
        "            for j in range(len(s)):\n",
        "                if i[j] != s[j]:\n",
        "                    g[j][j] = s[j]\n",
        "                else:\n",
        "                    g[j][j] = \"?\"\n",
        "        print(\"\\nSteps of Candidate Elimination Algorithm\", data.index(i) + 1)\n",
        "        print(s)\n",
        "        print(g)\n",
        "    gh = []\n",
        "    for i in g:\n",
        "        for j in i:\n",
        "            if j != '?':\n",
        "                gh.append(i)\n",
        "                break\n",
        "    print(\"\\nFinal specific hypothesis:\\n\", s)\n",
        "\n",
        "    print(\"\\nFinal general hypothesis:\\n\", gh)\n"
      ],
      "metadata": {
        "id": "c45lxtm9y40y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "X = X / np.amax(X, axis=0)  # maximum of X array longitudinally\n",
        "y = y / 100\n",
        "\n",
        "\n",
        "# Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Derivative of Sigmoid Function\n",
        "def derivatives_sigmoid(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "# Variable initialization\n",
        "epoch = 7000  # Setting training iterations\n",
        "lr = 0.1  # Setting learning rate\n",
        "inputlayer_neurons = 2  # number of features in data set\n",
        "hiddenlayer_neurons = 3  # number of hidden layers neurons\n",
        "output_neurons = 1  # number of neurons at output layer\n",
        "# weight and bias initialization\n",
        "wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))\n",
        "bh = np.random.uniform(size=(1, hiddenlayer_neurons))\n",
        "wout = np.random.uniform(size=(hiddenlayer_neurons, output_neurons))\n",
        "bout = np.random.uniform(size=(1, output_neurons))\n",
        "# draws a random range of numbers uniformly of dim x*y\n",
        "for i in range(epoch):\n",
        "    # Forward Propogation\n",
        "    hinp1 = np.dot(X, wh)\n",
        "    hinp = hinp1 + bh\n",
        "    hlayer_act = sigmoid(hinp)\n",
        "    outinp1 = np.dot(hlayer_act, wout)\n",
        "    outinp = outinp1 + bout\n",
        "    output = sigmoid(outinp)\n",
        "    # Backpropagation\n",
        "    EO = y - output\n",
        "    outgrad = derivatives_sigmoid(output)\n",
        "    d_output = EO * outgrad\n",
        "    EH = d_output.dot(wout.T)\n",
        "    hiddengrad = derivatives_sigmoid(hlayer_act)\n",
        "    d_hiddenlayer = EH * hiddengrad\n",
        "    wout += hlayer_act.T.dot(d_output) * lr\n",
        "    # bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
        "    wh += X.T.dot(d_hiddenlayer) * lr\n",
        "    # bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
        "print(\"Input: \\n\" + str(X))\n",
        "print(\"Actual Output: \\n\" + str(y))\n",
        "print(\"Predicted Output: \\n\", output)\n"
      ],
      "metadata": {
        "id": "Q5I0KHan0d_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "X = X/np.amax(X,axis=0) # maximum of X array longitudinally\n",
        "y = y/100\n",
        "\n",
        "#Sigmoid Function\n",
        "def sigmoid (x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Derivative of Sigmoid Function\n",
        "def derivatives_sigmoid(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "#Variable initialization\n",
        "epoch=5000                #Setting training iterations\n",
        "lr=0.1                    #Setting learning rate\n",
        "inputlayer_neurons = 2    #number of features in data set\n",
        "hiddenlayer_neurons = 3   #number of hidden layers neurons\n",
        "output_neurons = 1        #number of neurons at output layer\n",
        "\n",
        "#weight and bias initialization\n",
        "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
        "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
        "bout=np.random.uniform(size=(1,output_neurons))\n",
        "\n",
        "\n",
        "#draws a random range of numbers uniformly of dim x*y\n",
        "for i in range(epoch):\n",
        "\n",
        "#Forward Propogation\n",
        "    hinp1=np.dot(X,wh)\n",
        "    hinp=hinp1 + bh\n",
        "    hlayer_act = sigmoid(hinp)\n",
        "    outinp1=np.dot(hlayer_act,wout)\n",
        "    outinp= outinp1+ bout\n",
        "    output = sigmoid(outinp)\n",
        "\n",
        "#Backpropagation\n",
        "    EO = y-output\n",
        "    outgrad = derivatives_sigmoid(output)\n",
        "    d_output = EO* outgrad\n",
        "    EH = d_output.dot(wout.T)\n",
        "\n",
        "#how much hidden layer wts contributed to error\n",
        "    hiddengrad = derivatives_sigmoid(hlayer_act)\n",
        "    d_hiddenlayer = EH * hiddengrad\n",
        "\n",
        "# dotproduct of nextlayererror and currentlayerop\n",
        "    wout += hlayer_act.T.dot(d_output) *lr\n",
        "    wh += X.T.dot(d_hiddenlayer) *lr\n",
        "\n",
        "\n",
        "print(\"Input: \\n\" + str(X))\n",
        "print(\"Actual Output: \\n\" + str(y))\n",
        "print(\"Predicted Output: \\n\" ,output)"
      ],
      "metadata": {
        "id": "lSmoNLJhYLU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6th Program**"
      ],
      "metadata": {
        "id": "BGOCh_z4v1XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "msg = pd.read_csv('NBC.csv', names=['message', 'label'])\n",
        "print(\"Total Instances of Dataset: \", msg.shape[0])\n",
        "msg['labelnum'] = msg.label.map({'pos': 1, 'neg': 0})\n",
        "\n",
        "X = msg.message\n",
        "y = msg.labelnum\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n",
        "\n",
        "count_v = CountVectorizer()\n",
        "Xtrain_dm = count_v.fit_transform(Xtrain)\n",
        "Xtest_dm = count_v.transform(Xtest)\n",
        "\n",
        "df = pd.DataFrame(Xtrain_dm.toarray(), columns=count_v.get_feature_names_out())\n",
        "print(df[0:5])\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(Xtrain_dm, ytrain)\n",
        "pred = clf.predict(Xtest_dm)\n",
        "\n",
        "for doc, p in zip(Xtrain, pred):\n",
        "    p = 'pos' if p == 1 else 'neg'\n",
        "    print(\"%s -> %s\" % (doc, p))\n",
        "\n",
        "print('Accuracy Metrics: \\n')\n",
        "print('Accuracy: ', accuracy_score(ytest, pred))\n",
        "print('Recall: ', recall_score(ytest, pred))\n",
        "print('Precision: ', precision_score(ytest, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(ytest, pred))"
      ],
      "metadata": {
        "id": "a9BTc64g0lZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Load Data from CSV\n",
        "data = pd.read_csv('p-tennis.csv')\n",
        "print(\"The first 5 Values of data is :\\n\", data.head())\n",
        "\n",
        "# obtain train data and train output\n",
        "X = data.iloc[:, :-1]\n",
        "print(\"\\nThe First 5 values of the train data is\\n\", X.head())\n",
        "\n",
        "y = data.iloc[:, -1]\n",
        "print(\"\\nThe First 5 values of train output is\\n\", y.head())\n",
        "\n",
        "# convert them in numbers\n",
        "le_outlook = LabelEncoder()\n",
        "X.Outlook = le_outlook.fit_transform(X.Outlook)\n",
        "\n",
        "le_Temperature = LabelEncoder()\n",
        "X.Temperature = le_Temperature.fit_transform(X.Temperature)\n",
        "\n",
        "le_Humidity = LabelEncoder()\n",
        "X.Humidity = le_Humidity.fit_transform(X.Humidity)\n",
        "\n",
        "le_Windy = LabelEncoder()\n",
        "X.Windy = le_Windy.fit_transform(X.Windy)\n",
        "\n",
        "print(\"\\nNow the Train output is\\n\", X.head())\n",
        "\n",
        "le_PlayTennis = LabelEncoder()\n",
        "y = le_PlayTennis.fit_transform(y)\n",
        "print(\"\\nNow the Train output is\\n\",y)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)\n",
        "\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy is:\", accuracy_score(classifier.predict(X_test), y_test))"
      ],
      "metadata": {
        "id": "I1ugqGJQvxIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\tKmeans\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "print(iris)\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target)\n",
        "model =KMeans(n_clusters=3)\n",
        "model.fit(X_train,y_train)\n",
        "model.score\n",
        "print('K-Mean: ',metrics.accuracy_score(y_test,model.predict(X_test)))\n",
        "\n",
        "#-------Expectation and Maximization----------\n",
        "from sklearn.mixture import GaussianMixture\n",
        "model2 = GaussianMixture(n_components=3)\n",
        "model2.fit(X_train,y_train)\n",
        "model2.score\n",
        "print('EM Algorithm:',metrics.accuracy_score(y_test,model2.predict(X_test)))"
      ],
      "metadata": {
        "id": "Y0V9n23I_E-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import sklearn.metrics as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = pd.DataFrame(iris.data)\n",
        "X.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
        "Y = pd.DataFrame(iris.target)\n",
        "Y.columns = ['Targets']\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "colormap = np.array(['red', 'lime', 'black'])\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[Y.Targets], s=40)\n",
        "plt.title('Real Clustering')\n",
        "\n",
        "model1 = KMeans(n_clusters=3)\n",
        "model1.fit(X)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model1.labels_], s=40)\n",
        "plt.title('K Mean Clustering')\n",
        "plt.show()\n",
        "\n",
        "model2 = GaussianMixture(n_components=3)\n",
        "model2.fit(X)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model2.predict(X)], s=40)\n",
        "plt.title('EM Clustering')\n",
        "plt.show()\n",
        "\n",
        "print(\"Actual Target is:\\n\", iris.target)\n",
        "print(\"K Means:\\n\",model1.labels_)\n",
        "print(\"EM:\\n\",model2.predict(X))\n",
        "print(\"Accuracy of KMeans is \",sm.accuracy_score(Y,model1.labels_))\n",
        "print(\"Accuracy of EM is \",sm.accuracy_score(Y, model2.predict(X)))"
      ],
      "metadata": {
        "id": "19fdc4-B2Fi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "iris_data = iris.data\n",
        "iris_labels = iris.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_labels, test_size=0.20)\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "print('Confusion matrix is as follows')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print('Accuracy Metrics')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "oX_23W5D2UTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def local_regression(x0, X, Y, tau):\n",
        "    x0 = [1, x0]\n",
        "    X = [[1, i] for i in X]\n",
        "    X = np.asarray(X)\n",
        "    xw = (X.T) * np.exp(np.sum((X - x0) ** 2, axis=1) / (-2 * tau))\n",
        "    beta = np.linalg.pinv(xw @ X) @ xw @ Y @ x0\n",
        "    return beta\n",
        "\n",
        "def draw(tau):\n",
        "    prediction = [local_regression(x0, X, Y, tau) for x0 in domain]\n",
        "    plt.plot(X, Y, 'o', color='black')\n",
        "    plt.plot(domain, prediction, color='red')\n",
        "    plt.show()\n",
        "\n",
        "X = np.linspace(-3, 3, num=1000)\n",
        "domain = X\n",
        "Y = np.log(np.abs(X ** 2 - 1) + .5)\n",
        "\n",
        "draw(10)\n",
        "draw(0.1)\n",
        "draw(0.01)\n",
        "draw(0.001)"
      ],
      "metadata": {
        "id": "0qgUhfrcdTbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "# df_tennis = DataFrame.from\n",
        "df_tennis = pd.read_csv('ID3.csv')\n",
        "\n",
        "# df_tennis = DataFrame.from\n",
        "df_tennis = pd.read_csv('ID3.csv')\n",
        "\n",
        "\n",
        "# print(df_tennis)\n",
        "\n",
        "# Calculate the Entropy of given probability\n",
        "def entropy(probs):\n",
        "    import math\n",
        "    return sum([-prob * math.log(prob, 2) for prob in probs])\n",
        "\n",
        "\n",
        "def entropy_of_list(a_list):  # Entropy calculation of list of discrete val ues(YES / NO)\n",
        "    from collections import Counter\n",
        "    cnt = Counter(x for x in a_list)\n",
        "    print(\"No and Yes Classes:\", a_list.name, cnt)\n",
        "    num_instances = len(a_list) * 1.0\n",
        "    probs = [x / num_instances for x in cnt.values()]\n",
        "    return entropy(probs)  # Call Entropy:\n",
        "\n",
        "\n",
        "# The initial entropy of the YES/NO attribute for our dataset.\n",
        "# print(df_tennis['PlayTennis'])\n",
        "total_entropy = entropy_of_list(df_tennis['PlayTennis'])\n",
        "print(\"Entropy of given PlayTennis Data Set:\", total_entropy)\n",
        "\n",
        "\n",
        "def information_gain(df, split_attribute_name, target_attribute_name, trace=0):\n",
        "    print(\"Information Gain Calculation of \", split_attribute_name)\n",
        "    df_split = df.groupby(split_attribute_name)\n",
        "    '''\n",
        " Takes a DataFrame of attributes,and quantifies the entropy of a target\n",
        " attribute after performing a split along the values of another attribute.\n",
        " '''  # print(df_split.groups)\n",
        "    for name, group in df_split:\n",
        "        print(name)\n",
        "        print(group)\n",
        "    # Calculate Entropy for Target Attribute, as well as\n",
        "    # Proportion of Obs in Each Data-Split\n",
        "    nobs = len(df.index) * 1.0\n",
        "    # print(\"NOBS\",nobs)\n",
        "    df_agg_ent = df_split.agg({target_attribute_name: [entropy_of_list, lambda x: len(x) / nobs]})[\n",
        "        target_attribute_name]\n",
        "    # print(\"FAGGED\",df_agg_ent)\n",
        "    df_agg_ent.columns = ['Entropy', 'PropObservations']\n",
        "    # if traced: # helps understand what fxn is doing:\n",
        "    # Calculate Information Gain:\n",
        "    new_entropy = sum(df_agg_ent['Entropy'] * df_agg_ent['PropObservations'])\n",
        "    old_entropy = entropy_of_list(df[target_attribute_name])\n",
        "    return old_entropy - new_entropy\n",
        "\n",
        "\n",
        "# print('Info-gain for Outlook is :'+str( information_gain(df_tennis, 'Outlook', 'PlayTennis')),\"\\n\")\n",
        "# print('\\n Info-gain for Humidity is: ' + str( information_gain(df_tennis,'Humidity', 'PlayTennis')),\"\\n\")\n",
        "# print('\\n Info-gain for Wind is:' + str( information_gain(df_tennis, 'Wind', 'PlayTennis')),\"\\n\")\n",
        "# print('\\n Info-gain for Temperature is:' + str( information_gain(df_tennis, 'Temperature','PlayTennis')),\"\\n\")\n",
        "\n",
        "\n",
        "def id3(df, target_attribute_name, attribute_names, default_class=None):  # Tally target attribute\n",
        "    from collections import Counter\n",
        "    cnt = Counter(x for x in df[target_attribute_name])  # class of YES /NO\n",
        "    # First check: Is this split of the dataset homogeneous?\n",
        "    if len(cnt) == 1:\n",
        "        return next(iter(cnt))\n",
        "    # Second check: Is this split of the dataset empty?\n",
        "    # if yes, return a default value\n",
        "    elif df.empty or (not attribute_names):\n",
        "        return default_class\n",
        "        # Otherwise: This dataset is ready to be divvied up!\n",
        "    else:\n",
        "        # [index_of_max] # most common value  of  target  attribute in dataset\n",
        "        default_class = max(cnt.keys())\n",
        "        # Choose Best Attribute to split on:\n",
        "        gainz = [information_gain(df, attr, target_attribute_name)\n",
        "                 for attr in attribute_names]\n",
        "        index_of_max = gainz.index(max(gainz))\n",
        "        best_attr = attribute_names[index_of_max]\n",
        "        # Create an empty tree, to be populated in a moment\n",
        "        tree = {best_attr: {}}\n",
        "        remaining_attribute_names = [\n",
        "            i for i in attribute_names if i != best_attr]\n",
        "        # Split dataset\n",
        "        # On each split, recursively call this algorithm.\n",
        "        # populate the empty tree with subtrees, which\n",
        "        # are the result of the recursive call\n",
        "        for attr_val, data_subset in df.groupby(best_attr):\n",
        "            subtree = id3(data_subset, target_attribute_name,\n",
        "                          remaining_attribute_names, default_class)\n",
        "            tree[best_attr][attr_val] = subtree\n",
        "        return tree\n",
        "\n",
        "\n",
        "# Predicting Attributes\n",
        "attribute_names = list(df_tennis.columns)\n",
        "print(\"List of Attributes:\", attribute_names)\n",
        "attribute_names.remove('PlayTennis')  # Remove the class attribute\n",
        "print(\"Predicting Attributes:\", attribute_names)\n",
        "\n",
        "# Tree Construction\n",
        "\n",
        "tree = id3(df_tennis, 'PlayTennis', attribute_names)\n",
        "print(\"\\n\\nThe Resultant Decision Tree is :\\n\")\n",
        "pprint(tree)\n",
        "\n",
        "\n",
        "# Classification Accuracy\n",
        "def classify(instance, tree, default=None):\n",
        "    attribute = next(iter(tree))  # tree.keys()[0]\n",
        "    if instance[attribute] in tree[attribute].keys():\n",
        "        result = tree[attribute][instance[attribute]]\n",
        "        if isinstance(result, dict):  # this is a tree, delve deeper\n",
        "            return classify(instance, result)\n",
        "        else:\n",
        "            return result  # this is a label\n",
        "    else:\n",
        "        return default\n",
        "\n",
        "\n",
        "df_tennis['predicted'] = df_tennis.apply(classify, axis=1, args=(tree, 'No'))\n",
        "# classify func allows for a default arg: when tree doesn't have answered for a particular\n",
        "# combination of attribute-values, we can use 'no' as the default guess\n",
        "print('Accuracy is:' + str(sum(df_tennis['PlayTennis'] ==\n",
        "      df_tennis['predicted']) / (1.0 * len(df_tennis.index))))\n",
        "df_tennis[['PlayTennis', 'predicted']]\n"
      ],
      "metadata": {
        "id": "zdUEMnh6h_y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}